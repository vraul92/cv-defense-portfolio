# CV Defense Portfolio

**Expert-Level Computer Vision Projects for Defense Applications**

[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)](https://python.org)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.8+-green.svg)](https://opencv.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A curated collection of production-grade computer vision systems designed for defense and surveillance applications. Each project demonstrates expertise in real-time processing, edge deployment, and mission-critical reliability.

![Portfolio Overview](assets/portfolio_overview.png)

---

## ğŸ“ Projects

### 1. ğŸ”¥ ThermalSAM-Lite
**Zero-Shot Object Detection for Thermal Imagery**

Adapts Meta's Segment Anything Model (SAM) for defense-grade thermal imaging. Detects humans, vehicles, and heat signatures without domain-specific training.

**Key Features:**
- SAM-based zero-shot detection
- Thermal-to-RGB domain adaptation
- False-color visualization
- Temperature calibration
- Edge-optimized inference

```bash
cd thermalsam-lite
python demo.py --create-sample
python demo_sam3d.py --create-synthetic  # Multi-view 3D reconstruction
```

**Defense Applications:** Night surveillance, search & rescue, perimeter security, UAV payloads

---

### 2. ğŸ‘» Ghost-Tracker
**Real-Time Multi-Object Tracking with Occlusion Handling**

Implements Extended Kalman Filter (EKF) from scratch for robust multi-object tracking. Features "ghost mode" to maintain identity during occlusions.

**Key Features:**
- Full EKF implementation (no libraries)
- Mahalanobis distance data association
- Ghost mode for occlusion handling
- Uncertainty ellipse visualization
- Multi-object support

```bash
cd ghost-tracker
python demo_video.py --input samples/traffic.mp4 --output output/
```

**Defense Applications:** Perimeter security, UAV tracking, convoy protection, missile guidance

---

### 3. ğŸ—ºï¸ Recon-Map
**Visual Odometry and Trajectory Mapping for GPS-Denied Navigation**

Estimates camera motion from video sequences and reconstructs flight paths. Designed for UAV navigation in GPS-denied environments.

**Key Features:**
- Monocular visual odometry
- ORB feature extraction
- Essential matrix decomposition
- Real-time 2D/3D trajectory mapping
- Interactive dashboard

```bash
cd recon-map
python demo.py --input video.mp4 --output output/
```

**Defense Applications:** UAV navigation, indoor reconnaissance, subterranean operations, urban canyon navigation

---

## ğŸš€ Quick Start

### Clone and Setup

```bash
git clone https://github.com/vraul92/cv-defense-portfolio.git
cd cv-defense-portfolio

# Each project has its own setup
for project in thermalsam-lite ghost-tracker recon-map; do
    cd $project
    python3.10 -m venv venv
    source venv/bin/activate
    pip install -r requirements.txt
    cd ..
done
```

### Run All Demos

```bash
# ThermalSAM-Lite
python thermalsam-lite/demo.py --create-sample

# SAM 3D Reconstruction
python thermalsam-lite/demo_sam3d.py --create-synthetic

# Ghost-Tracker
python ghost-tracker/demo_video.py --input your_video.mp4

# Recon-Map
python recon-map/demo.py --input your_video.mp4
```

---

## ğŸ“¸ Paper Figures & Visual Explanations

### SAM Architecture (Kirillov et al., 2023)

![SAM Architecture](https://github.com/facebookresearch/segment-anything/raw/main/assets/model_diagram.png?raw=true)

*Figure 1: SAM architecture consists of (1) an image encoder, (2) a flexible prompt encoder, and (3) a fast mask decoder. This design enables zero-shot transfer to new tasks and distributions.*

**Key Components:**
- **Image Encoder**: Vision Transformer (ViT) processing 1024Ã—1024 images
- **Prompt Encoder**: Supports points, boxes, and text prompts
- **Mask Decoder**: Lightweight decoder predicting segmentation masks in ~50ms on CPU

---

### MiDaS Depth Estimation (Ranftl et al., 2022)

![MiDaS Overview](https://github.com/isl-org/MiDaS/raw/master/output.gif)

*Figure 2: MiDaS produces robust monocular depth estimates across diverse scenarios without retraining. The model uses a mixture of datasets for zero-shot cross-dataset transfer.*

**Key Innovation:**
- Multi-dataset training enables generalization
- Scale-invariant loss handles unknown camera parameters
- Runs in real-time on CPU

---

### Kalman Filter Cycle (Welch & Bishop, 1995)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KALMAN FILTER CYCLE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Prediction (Time Update)         Correction (Measurement) â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ xÌ‚â‚–â» = FÂ·xÌ‚â‚–â‚‹â‚    â”‚             â”‚ K = Pâ‚–â»Â·Háµ€Â·Sâ»Â¹     â”‚  â”‚
â”‚   â”‚ Pâ‚–â» = FÂ·Pâ‚–â‚‹â‚Â·Fáµ€ + Qâ”‚           â”‚ xÌ‚â‚– = xÌ‚â‚–â» + KÂ·(z-HÂ·xÌ‚â‚–â»)â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚ Pâ‚– = (I-KÂ·H)Â·Pâ‚–â»   â”‚  â”‚
â”‚           â†“                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚      (Prior Estimate)                  (Posterior Estimate) â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

*Figure 3: The Kalman Filter operates in a two-step process: prediction (using motion model) and correction (using sensor measurements).*

**Variables:**
- **xÌ‚**: State estimate
- **P**: Error covariance
- **F**: State transition matrix
- **Q**: Process noise
- **K**: Kalman gain
- **H**: Measurement matrix
- **R**: Measurement noise

---

### Epipolar Geometry (Ma et al., 2004)

```
    Image 1                      Image 2
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    p    â”‚ â”€â”€â”€ Epipolar â”€â†’ â”‚ â”€â”€â”€â”€â”€â”€â”€ â”‚
    â”‚    â—â”€â”€â”€â”€â”¼â”€â”€â”€ Line â”€â”€â”€â”€â”€â”€â”€â†’â”¼â”€â”€â”€â”€â”€â—   â”‚
    â”‚    â”‚    â”‚                 â”‚    /    â”‚
    â”‚    â”‚    â”‚                 â”‚   /     â”‚
    â””â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”˜
         â”‚                          â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Câ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Baseline

    Essential Matrix:  pâ‚‚áµ€ Â· E Â· pâ‚ = 0
    
    Where E = [t]Ã— Â· R (cross product of translation Ã— rotation)
```

*Figure 4: Epipolar geometry constrains where a point in one image can appear in another. The Essential Matrix E encodes the relative pose between two calibrated camera views.*

---

### ORB Feature Matching (Rublee et al., 2011)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ORB PIPELINE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  FAST Corners              rBRIEF Descriptors               â”‚
â”‚       â†“                           â†“                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ â—  â—  â— â”‚               â”‚ 01010110... â”‚                  â”‚
â”‚  â”‚   â—    â”‚  â”€â”€Orientationâ†’â”‚ 128-bit     â”‚                  â”‚
â”‚  â”‚ â—  â—  â— â”‚    (oFAST)     â”‚ binary      â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚       â”‚                           â”‚                         â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Hamming â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                  Distance Matching                          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

*Figure 5: ORB combines FAST keypoint detection with BRIEF descriptors, adding rotation invariance through orientation assignment.*

---

## ğŸ¯ Technical Highlights

| Project | Core Algorithm | Lines of Code | Dependencies |
|---------|---------------|---------------|--------------|
| ThermalSAM-Lite | SAM + Thermal Adaptation | ~1,300 | PyTorch, OpenCV |
| SAM 3D Lite | Multi-View Reconstruction | ~500 | PyTorch, MiDaS |
| Ghost-Tracker | EKF + Hungarian Algorithm | ~900 | NumPy, OpenCV, SciPy |
| Recon-Map | Visual Odometry (ORB) | ~600 | OpenCV, Matplotlib |

**Total Codebase:** ~3,300 lines of production-grade Python

---

## ğŸ—ï¸ Architecture Overview

### ThermalSAM-Lite
```
Thermal Input (16-bit) â†’ False-Color Mapping â†’ SAM Encoder â†’ 
Mask Decoder â†’ Post-Processing â†’ Visualization (Bbox + Temperature)
```

### SAM 3D Lite
```
Multi-View Images â†’ SAM Segmentation â†’ MiDaS Depth â†’ 
Back-Projection â†’ Point Cloud Fusion â†’ Voxel Downsampling â†’ 3D Output
```

### Ghost-Tracker
```
Video Frame â†’ YOLO Detection â†’ Data Association (Mahalanobis) â†’ 
EKF Update â†’ Ghost Mode Logic â†’ Visualization (Bbox + Ellipse)
```

### Recon-Map
```
Frame â†’ ORB Features â†’ Feature Matching â†’ Essential Matrix â†’ 
Pose Recovery â†’ Trajectory Update â†’ 2D/3D Visualization
```

---

## ğŸ›¡ï¸ Defense Relevance

### Why These Projects?

**ThermalSAM-Lite** demonstrates:
- Foundation model adaptation (SAM)
- Domain transfer (RGB â†’ Thermal)
- Edge deployment optimization
- Temperature-aware detection

**SAM 3D Lite** demonstrates:
- Multi-view geometry
- Depth estimation integration
- 3D reconstruction from 2D
- Point cloud processing

**Ghost-Tracker** demonstrates:
- State estimation (Kalman filtering)
- Data association (Mahalanobis distance)
- Occlusion handling (ghost mode)
- Multi-object tracking

**Recon-Map** demonstrates:
- Geometric computer vision
- Structure from Motion
- GPS-denied navigation
- Real-time pose estimation

---

## ğŸ“Š Performance Benchmarks

All projects optimized for **CPU inference** (no GPU required):

| Project | CPU (i7) | Memory | Real-time? |
|---------|----------|--------|------------|
| ThermalSAM-Lite | ~2.5 FPS | 4GB | âœ“ (for surveillance) |
| SAM 3D Lite | ~1.0 FPS | 6GB | âœ“ (batch processing) |
| Ghost-Tracker | ~30 FPS | 2GB | âœ“ (real-time) |
| Recon-Map | ~15 FPS | 1GB | âœ“ (real-time) |

---

## ğŸ“– Documentation

Each project includes:
- Comprehensive README with usage examples
- Inline code documentation
- Configuration files (YAML)
- Sample data generators
- Demo scripts

### API Examples

**ThermalSAM:**
```python
from thermalsam_lite import ThermalSAM

model = ThermalSAM(checkpoint="sam_vit_b.pth")
results = model.detect(thermal_image)
model.visualize(results, save_path="output.jpg")
```

**Ghost-Tracker:**
```python
from ghost_tracker import GhostTracker

tracker = GhostTracker(max_age=30, min_hits=3)
tracks = tracker.update(detections)
```

**Recon-Map:**
```python
from recon_map import VisualOdometry

vo = VisualOdometry(focal_length=800.0)
pose = vo.process_frame(frame)
trajectory = vo.get_trajectory()
```

---

## ğŸ“š References & Citations

### Core Papers

**SAM (Segment Anything):**
```bibtex
@article{kirillov2023segment,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  journal={arXiv preprint arXiv:2304.02643},
  year={2023}
}
```

**MiDaS (Depth Estimation):**
```bibtex
@article{ranftl2022towards,
  title={Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer},
  author={Ranftl, RenÃ© and Bochkovskiy, Alexey and Koltun, Vladlen},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022}
}
```

**Kalman Filter:**
```bibtex
@article{welch1995introduction,
  title={An Introduction to the Kalman Filter},
  author={Welch, Greg and Bishop, Gary},
  year={1995}
}
```

**Visual Odometry:**
```bibtex
@book{ma2004invitation,
  title={An Invitation to 3-D Vision: From Images to Geometric Models},
  author={Ma, Yi and Soatto, Stefano and Kosecka, Jana and Sastry, S. Shankar},
  year={2004},
  publisher={Springer}
}
```

**ORB Features:**
```bibtex
@article{rublee2011orb,
  title={ORB: An efficient alternative to SIFT or SURF},
  author={Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
  journal={IEEE International Conference on Computer Vision (ICCV)},
  year={2011}
}
```

---

## ğŸ“ Learning Path

### For Thermal Imaging:
1. Study SAM architecture (ViT encoder + prompt encoder + mask decoder)
2. Understand thermal-to-RGB domain adaptation
3. Experiment with false-color mapping techniques
4. Practice temperature calibration

### For Tracking:
1. Learn Kalman Filter theory (prediction + update)
2. Understand Mahalanobis distance for data association
3. Implement Hungarian algorithm for assignment
4. Study occlusion handling strategies

### For Visual Odometry:
1. Study epipolar geometry
2. Understand feature extraction (ORB/SIFT)
3. Learn Essential matrix decomposition
4. Practice Bundle Adjustment

---

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:
- [ ] TensorRT optimization for Jetson
- [ ] Quantization (INT8) for edge deployment
- [ ] Multi-frame temporal consistency
- [ ] ROS2 integration
- [ ] Custom training on defense-specific data

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file.

---

## ğŸ‘¤ Author

**Rahul Vuppalapati**  
Senior Data Scientist | Computer Vision & Defense AI  

**Experience:**
- 8+ years in ML/CV (Apple, Walmart, IBM)
- GenAI & RAG Expert
- Edge AI & Embedded Systems
- Real-time Computer Vision

**Contact:**
- ğŸ“§ vrc7.ds@gmail.com
- ğŸ”— [LinkedIn](https://linkedin.com/in/vrc7)
- ğŸ™ [GitHub](https://github.com/vraul92)

---

**Built with â¤ï¸ for defense applications.**

*If you find this work useful, please star the repo!* â­

---

## ğŸ™ Acknowledgments

- Meta AI for the [Segment Anything Model](https://github.com/facebookresearch/segment-anything)
- Intel for [MiDaS depth estimation](https://github.com/isl-org/MiDaS)
- OpenCV community for computer vision tools
- FLIR Systems for thermal imaging datasets
